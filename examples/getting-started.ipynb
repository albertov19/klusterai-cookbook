{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08513ae-21ba-4e77-9bad-2bc5c51f4d68",
   "metadata": {},
   "source": [
    "# Getting started with the kluster.ai API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d0174-a10b-4eb0-a811-dd11deba2f6d",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kluster-ai/klusterai-cookbook/blob/main/examples/getting-started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2c4269-59e8-456c-8939-35cf0bdda3c9",
   "metadata": {},
   "source": [
    "Welcome to the kluster.ai getting started notebook!\n",
    "\n",
    "<a href=\"https://kluster.ai/\" target=\"_blank\">kluster.ai</a> is a high-performance platform designed to make large-scale AI workloads accessible, efficient, and affordable. Our Adaptive Inference API is an asynchronous service with higher rate limits, predictable turnaround times, and unmatched value. It enables a variety of use cases such as summarization, classification, translation, and much more, all without the need to manage infrastructure. \n",
    "\n",
    "This notebook is designed to help you get started quickly. It walks you through the essential code snippets from the <a href=\"https://docs.kluster.ai/get-started/api/\" target=\"_blank\">Getting started guide</a>, all in one place.\n",
    "\n",
    "By running this notebook, you’ll:\n",
    "- Learn how to use the API.\n",
    "- Submit a simple Adaptive Inference request using our open source LLMs.\n",
    "- Understand how to handle and interpret the API’s responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9adbe5d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e4c18",
   "metadata": {},
   "source": [
    "This step ensures that the openai Python library is installed or updated to the required version. This library will serve as the client for interacting with the kluster.ai API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28f85f2a-4ffc-4180-b439-906493833a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai>=1.0.0 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (1.54.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from openai>=1.0.0) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from openai>=1.0.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from openai>=1.0.0) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from openai>=1.0.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from openai>=1.0.0) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from openai>=1.0.0) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from openai>=1.0.0) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from openai>=1.0.0) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai>=1.0.0) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai>=1.0.0) (1.2.2)\n",
      "Requirement already satisfied: certifi in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0.0) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.0.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/erichenderson/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->openai>=1.0.0) (2.23.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"openai>=1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b089f9c-e3fd-4411-966e-fabe5d392e4c",
   "metadata": {},
   "source": [
    "## Creating inference jobs as JSONL files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a9b10",
   "metadata": {},
   "source": [
    "This step defines a collection of requests for the API to process. Each request includes a unique identifier (`custom_id`), the HTTP method (`POST`), the chat completions endpoint (`/v1/chat/completions`) and a body field that contains the request you want to send to the chat completions endpoint toghether with the `model` to be used and the conversational context (\"messages\"). These tasks are saved as a JSON Lines (`.jsonl`) file for efficient handling of multiple requests in a single upload.\n",
    "\n",
    "You'll have to enter your personal kluster.ai API key (make sure it has no blank spaces). Remember to create a key in <a href=\"https://platform.kluster.ai/apikeys\" target=\"_blank\">platform.kluster.ai</a>, if you don't have one yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb21d1-df49-4784-a5de-f1a7367dd481",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.kluster.ai/v1\",  \n",
    "    api_key=\"sk_GNm8I8_JBwpC6TB2JEeWuVdBG_0CKhz5j677jd9cdB8\", # Replace with your actual API key\n",
    "    #api_key=\"INSERT_API_KEY\", # Replace with your actual API key\n",
    ")\n",
    "\n",
    "tasks = [{\n",
    "        \"custom_id\": \"request-1\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is the capital of Argentina?\"},\n",
    "            ],\n",
    "            \"max_tokens\": 1000,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"custom_id\": \"request-2\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"klusterai/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a maths tutor.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Explain the Pythagorean theorem.\"},\n",
    "            ],\n",
    "            \"max_tokens\": 1000,\n",
    "        },\n",
    "    }\n",
    "    # Additional tasks can be added here\n",
    "]\n",
    "\n",
    "# Save tasks to a JSONL file (newline-delimited JSON)\n",
    "file_name = \"my_inference_test.jsonl\"\n",
    "with open(file_name, \"w\") as file:\n",
    "    for task in tasks:\n",
    "        file.write(json.dumps(task) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb6a52-2cef-45ae-8e3e-fe31491c4ca2",
   "metadata": {},
   "source": [
    "## Uploading Adaptive Inference job files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1518be55",
   "metadata": {},
   "source": [
    "This step uploads the input JSONL file to kluster.ai via the API. Once the file is uploaded, the API assigns a unique file ID. This ID is essential for subsequent steps, as it allows you to specify which file the batch job should use for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a7dd436-f8e2-4a93-ba22-fddbdc0b4aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '67533eff1d7be1b5a7507b75',\n",
       " 'bytes': 602,\n",
       " 'created_at': 1733508863,\n",
       " 'filename': '6750b85c7da9ad513c97bea1/02d026ed-0742-46a8-9ff9-8fdaa5ea768f-4e3e0531-deab-4489-8da1-5df2466bd176',\n",
       " 'object': 'file',\n",
       " 'purpose': 'batch'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_input_file = client.files.create(\n",
    "    file=open(file_name, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "inference_input_file.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d0e43-f222-4024-ba6a-8564d2aca71f",
   "metadata": {},
   "source": [
    "## Submit your Adaptive Inference job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e53475",
   "metadata": {},
   "source": [
    "This step starts your job by providing the uploaded file ID and setting the endpoint and completion window, initiating the Adaptive Inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4537dd26-2058-415f-8b0c-cab75604e455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '67533f6d1d7be1b5a7507be5',\n",
       " 'completion_window': '24h',\n",
       " 'created_at': 1733508973,\n",
       " 'endpoint': '/v1/chat/completions',\n",
       " 'input_file_id': '67533eff1d7be1b5a7507b75',\n",
       " 'object': 'batch',\n",
       " 'status': 'pre_schedule',\n",
       " 'completed_at': None,\n",
       " 'errors': [],\n",
       " 'expires_at': 1733595373,\n",
       " 'failed_at': None,\n",
       " 'finalizing_at': None,\n",
       " 'in_progress_at': None,\n",
       " 'metadata': None,\n",
       " 'request_counts': {'completed': 0, 'failed': 0, 'total': 0}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_request = client.batches.create(\n",
    "    input_file_id=inference_input_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    ")\n",
    "\n",
    "inference_request.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca0141-731a-4751-8418-77dff33adadf",
   "metadata": {},
   "source": [
    "## Monitor job progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ed74c",
   "metadata": {},
   "source": [
    "In this step, the job status is checked repeatedly to track its progress. You’ll see updates on the overall status and the number of completed tasks until the job is finished, failed, or cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c15527d-5bf0-48c8-b31c-53d19934e0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: in_progress\n",
      "Completed tasks: 1 / 2\n",
      "Job status: in_progress\n",
      "Completed tasks: 1 / 2\n",
      "Job status: in_progress\n",
      "Completed tasks: 1 / 2\n",
      "Job status: completed\n",
      "Completed tasks: 2 / 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '67533f6d1d7be1b5a7507be5',\n",
       " 'completion_window': '24h',\n",
       " 'created_at': 1733508973,\n",
       " 'endpoint': '/v1/chat/completions',\n",
       " 'input_file_id': '67533eff1d7be1b5a7507b75',\n",
       " 'object': 'batch',\n",
       " 'status': 'completed',\n",
       " 'completed_at': 1733509004,\n",
       " 'errors': [],\n",
       " 'expires_at': 1733595373,\n",
       " 'failed_at': None,\n",
       " 'finalizing_at': 1733509004,\n",
       " 'in_progress_at': 1733508973,\n",
       " 'metadata': None,\n",
       " 'output_file_id': '67533f8c2ab49d0df6b4a583',\n",
       " 'request_counts': {'completed': 2, 'failed': 0, 'total': 2}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Poll the job's status until it's complete\n",
    "while True:\n",
    "    inference_status = client.batches.retrieve(inference_request.id)\n",
    "    print(\"Job status: {}\".format(inference_status.status))\n",
    "    print(\n",
    "        f\"Completed tasks: {inference_status.request_counts.completed} / {inference_status.request_counts.total}\"\n",
    "    )\n",
    "\n",
    "    if inference_status.status.lower() in [\"completed\", \"failed\", \"cancelled\"]:\n",
    "        break\n",
    "\n",
    "    time.sleep(10)  # Wait for 10 seconds before checking again\n",
    "\n",
    "inference_status.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa97a0-03d6-45d7-95f8-947198a0ef8e",
   "metadata": {},
   "source": [
    "## Retrieve results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734ea16",
   "metadata": {},
   "source": [
    "In this step, the results of the job are retrieved if it completed successfully. The output is downloaded and saved to a local file for you to review. If the job failed, the status will indicate the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66542b1d-85aa-4350-9551-bd40db7f56ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to inference_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Check if the job completed successfully\n",
    "if inference_status.status.lower() == \"completed\":\n",
    "    # Retrieve the results\n",
    "    result_file_id = inference_status.output_file_id\n",
    "    results = client.files.content(result_file_id).content\n",
    "\n",
    "    # Save results to a file\n",
    "    result_file_name = \"inference_results.jsonl\"\n",
    "    with open(result_file_name, \"wb\") as file:\n",
    "        file.write(results)\n",
    "    print(f\"Results saved to {result_file_name}\")\n",
    "else:\n",
    "    print(f\"Job failed with status: {inference_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73016a7-24af-47e2-aa35-a71f39300a88",
   "metadata": {},
   "source": [
    "## List all Adaptive Inference jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac081d6e",
   "metadata": {},
   "source": [
    "This step lists the most recent jobs, providing an overview of their statuses and details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1063f7f1-1cc3-4501-8293-9e1962a70e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erichenderson/Library/Python/3.9/lib/python/site-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `Errors` but got `list` with value `[]` - serialized value may not be as expected\n",
      "  Expected `Errors` but got `list` with value `[]` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': '67533f6d1d7be1b5a7507be5',\n",
       "   'completion_window': '24h',\n",
       "   'created_at': 1733508973,\n",
       "   'endpoint': '/v1/chat/completions',\n",
       "   'input_file_id': '67533eff1d7be1b5a7507b75',\n",
       "   'object': 'batch',\n",
       "   'status': 'completed',\n",
       "   'completed_at': 1733509004,\n",
       "   'errors': [],\n",
       "   'expires_at': 1733595373,\n",
       "   'failed_at': None,\n",
       "   'finalizing_at': 1733509004,\n",
       "   'in_progress_at': 1733508973,\n",
       "   'metadata': None,\n",
       "   'output_file_id': '67533f8c2ab49d0df6b4a583',\n",
       "   'request_counts': {'completed': 2, 'failed': 0, 'total': 2}},\n",
       "  {'id': '67533f041d7be1b5a7507b7b',\n",
       "   'completion_window': '24h',\n",
       "   'created_at': 1733508868,\n",
       "   'endpoint': '/v1/chat/completions',\n",
       "   'input_file_id': '67533eff1d7be1b5a7507b75',\n",
       "   'object': 'batch',\n",
       "   'status': 'completed',\n",
       "   'completed_at': 1733508903,\n",
       "   'errors': [],\n",
       "   'expires_at': 1733595268,\n",
       "   'failed_at': None,\n",
       "   'finalizing_at': 1733508903,\n",
       "   'in_progress_at': 1733508868,\n",
       "   'metadata': None,\n",
       "   'output_file_id': '67533f272ab49d0df6b4a561',\n",
       "   'request_counts': {'completed': 2, 'failed': 0, 'total': 2}}],\n",
       " 'object': 'list',\n",
       " 'first_id': '67533f6d1d7be1b5a7507be5',\n",
       " 'last_id': '67533f041d7be1b5a7507b7b',\n",
       " 'has_more': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.batches.list(limit=2).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9f1db-c226-4529-850b-f07fb4f6d695",
   "metadata": {},
   "source": [
    "## Cancelling an Adaptive Inference job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61d05d",
   "metadata": {},
   "source": [
    "To cancel a job that is currently in progress, invoke the cancel endpoint by providing the request ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38ba4b1a-f527-4adc-a20d-d94afacf52fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'message': \"Unknown error occurred: Body cannot be empty when content-type is set to 'application/json'\", 'errorCode': -1}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalServerError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inference_request\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m67533e071d7be1b5a75073d1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/resources/batches.py:226\u001b[0m, in \u001b[0;36mBatches.cancel\u001b[0;34m(self, batch_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_id:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a non-empty value for `batch_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/batches/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/cancel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1043\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1093\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1059\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1058\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1059\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1062\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1063\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1068\u001b[0m )\n",
      "\u001b[0;31mInternalServerError\u001b[0m: Error code: 500 - {'message': \"Unknown error occurred: Body cannot be empty when content-type is set to 'application/json'\", 'errorCode': -1}"
     ]
    }
   ],
   "source": [
    "inference_request.id=\"67533e071d7be1b5a75073d1\"\n",
    "client.batches.cancel(inference_request.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926edc8-b270-4f69-94f6-7745b5606080",
   "metadata": {},
   "source": [
    "## List supported models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149225c5",
   "metadata": {},
   "source": [
    "Find the right model for your job by first checking the list models endpoint. Choose from our range of models, optimized for different performance needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5be116c1-677d-4f3a-be86-c4b76f1284dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [{'id': 'klusterai/Meta-Llama-3.1-405B-Instruct-Turbo',\n",
       "   'created': 1731336418,\n",
       "   'object': 'model',\n",
       "   'owned_by': 'klusterai'},\n",
       "  {'id': 'klusterai/Meta-Llama-3.1-70B-Instruct-Turbo',\n",
       "   'created': 1731336610,\n",
       "   'object': 'model',\n",
       "   'owned_by': 'klusterai'},\n",
       "  {'id': 'klusterai/Meta-Llama-3.1-8B-Instruct-Turbo',\n",
       "   'created': 1731336610,\n",
       "   'object': 'model',\n",
       "   'owned_by': 'klusterai'}],\n",
       " 'object': 'list'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list().to_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
