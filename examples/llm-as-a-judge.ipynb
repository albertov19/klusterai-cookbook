{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be189fde-4e5b-4f80-bae1-ded86a5075a7",
   "metadata": {
    "id": "be189fde-4e5b-4f80-bae1-ded86a5075a7"
   },
   "source": [
    "# Evaluating LLM performance without ground truth using an LLM judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a77d9",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kluster-ai/klusterai-cookbook/blob/main/examples/prompt-engineering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a788f-a618-42a2-98c1-3d0e68ff766c",
   "metadata": {},
   "source": [
    "In our previous notebook we explored the idea of selecting the best model to perform a classification task. We did that by calculating the accuracy of each model based on a ground truth label. In real life applications, though, the ground truth is not always available and to create one we might depend on human anotation which is timeconsuming and costly. Even more if our model is predicting large volumes of data. \n",
    "\n",
    "In this notebook we'll explore the idea of leveraging the use of an LLM as a judge to evaluate another LLM's answer on a classification task and provide an evaluating base to understand how the base model is performing. \n",
    "\n",
    "Using the IMDb Top 1000 dataset, we will request the base model to classify the genre of the movies based on their description. Then we will request a different model to evaluate the first model's answer to decide if it's correct or not. As in this case we chose the IMDb 1000 Movie dataset which actually provides the ground truth we'll be able to check how good is our judge compared with the original ground truth that comes with the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xU1WBQJ7Uh09",
   "metadata": {
    "id": "xU1WBQJ7Uh09"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace7b9c-eb77-4f6a-a3c2-eb75581ed427",
   "metadata": {},
   "source": [
    "Enter your personal kluster.ai API key (make sure it has no blank spaces). Remember to <a href=\"https://platform.kluster.ai/signup\" target=\"_blank\">sign up</a> if you don't have one yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4911e5eb-7463-4a6b-8a99-9313cecd9d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your kluster.ai API key:  ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "# Enter you personal kluster.ai API key (make sure in advance it has no blank spaces)\n",
    "api_key = getpass(\"Enter your kluster.ai API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e5559-69c7-4274-8700-a7d94791e9db",
   "metadata": {
    "id": "650e5559-69c7-4274-8700-a7d94791e9db"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc3d475-8f49-4fc4-9a5e-c6eb6866d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89a4feb-37a9-430d-a742-cd58495b4eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "from IPython.display import clear_output, display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', 1000, 'display.width', 1000, 'display.max_rows',1000, 'display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "zG9y_WO5rYaj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.kluster.ai/v1\",\n",
    "    api_key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "udPtLfTaisSw",
   "metadata": {
    "id": "udPtLfTaisSw"
   },
   "source": [
    "## 1. Perform a classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "yC9wJlV4rwOh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Series_Title</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Overview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Wait Until Dark</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>A recently blinded woman is terrorized by a trio of thugs while they search for a heroin-stuffed doll they believe is in her apartment.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>Guess Who's Coming to Dinner</td>\n",
       "      <td>Comedy, Drama</td>\n",
       "      <td>A couple's attitudes are challenged when their daughter introduces them to her African-American fianc\u001a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>Bonnie and Clyde</td>\n",
       "      <td>Action, Biography, Crime</td>\n",
       "      <td>Bored waitress Bonnie Parker falls in love with an ex-con named Clyde Barrow and together they start a violent crime spree through the country, stealing cars and robbing banks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Series_Title                     Genre                                                                                                                                                                          Overview\n",
       "700               Wait Until Dark                  Thriller                                           A recently blinded woman is terrorized by a trio of thugs while they search for a heroin-stuffed doll they believe is in her apartment.\n",
       "701  Guess Who's Coming to Dinner             Comedy, Drama                                                                           A couple's attitudes are challenged when their daughter introduces them to her African-American fianc\u001a.\n",
       "702              Bonnie and Clyde  Action, Biography, Crime  Bored waitress Bonnie Parker falls in love with an ex-con named Clyde Barrow and together they start a violent crime spree through the country, stealing cars and robbing banks."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMDB Top 1000 dataset:\n",
    "url = \"https://raw.githubusercontent.com/kluster-ai/klusterai-cookbook/refs/heads/main/data/imdb_top_1000.csv\"\n",
    "urllib.request.urlretrieve(url,filename='imdb_top_1000.csv')\n",
    "\n",
    "# Load and process the dataset based on URL content\n",
    "df = pd.read_csv('imdb_top_1000.csv', usecols=['Series_Title', 'Overview', 'Genre']).tail(300)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff66e40d-1b4a-4f64-aa93-022b627be483",
   "metadata": {
    "id": "Ew-R24Ltp5EW"
   },
   "source": [
    "#### Create the Batch File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fVtwyqZ_nEq7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tasks(df, task_type, system_prompt, model):\n",
    "    tasks = []\n",
    "    for index, row in df.iterrows():\n",
    "        if task_type == 'assistant':\n",
    "            content = row['Overview']\n",
    "        elif task_type == 'judge':\n",
    "            content = f'''\n",
    "            Movie Description: {row['Overview']}.\n",
    "            Available Genres: Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western\n",
    "            LLM answer: \"{row['predicted_genre']}\"\n",
    "            '''\n",
    "        \n",
    "        task = {\n",
    "            \"custom_id\": f\"{task_type}-{index}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": model,\n",
    "                \"temperature\": 0,\n",
    "                \"response_format\": {\"type\": \"json_object\"},\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": content},\n",
    "                ],\n",
    "            }\n",
    "        }\n",
    "        tasks.append(task)\n",
    "    return tasks\n",
    "\n",
    "def save_tasks(tasks, task_type):\n",
    "    filename = f\"batch_tasks_{task_type}.jsonl\"\n",
    "    with open(filename, 'w') as file:\n",
    "        for task in tasks:\n",
    "            file.write(json.dumps(task) + '\\n')\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "qNhmrmHdnp7g",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSISTANT_PROMPT = '''\n",
    "    You are a helpful assitant that classifies movie genres based on the movie description. Choose one of the following options: \n",
    "    Action, Adventure, Animation, Biography, Comedy, Crime, Drama, Family, Fantasy, Film-Noir, History, Horror, Music, Musical, Mystery, Romance, Sci-Fi, Sport, Thriller, War, Western.\n",
    "    Provide your response as a single word with the matching genre. Don't include punctuation.\n",
    "    '''\n",
    "\n",
    "task_list = create_tasks(df, system_prompt=ASSISTANT_PROMPT, model=\"klusterai/Meta-Llama-3.1-8B-Instruct-Turbo\", task_type='assistant')\n",
    "filename = save_tasks(task_list, task_type='assistant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xArKu7-sqSiR",
   "metadata": {
    "id": "xArKu7-sqSiR"
   },
   "source": [
    "#### Upload Batch File to kluster.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "l5eu5UyAnEtk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batch job for batch_tasks_assistant.jsonl\n"
     ]
    }
   ],
   "source": [
    "def create_batch_job(file_name):\n",
    "    print(f\"Creating batch job for {file_name}\")\n",
    "    batch_file = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_job = client.batches.create(\n",
    "        input_file_id=batch_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\"\n",
    "    )\n",
    "\n",
    "    return batch_job\n",
    "\n",
    "job = create_batch_job(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e-ujphILqepu",
   "metadata": {
    "id": "e-ujphILqepu"
   },
   "source": [
    "#### Check Job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "SuH0CfoqjP3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Assistant job completed!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def parse_json_objects(data_string):\n",
    "    if isinstance(data_string, bytes):\n",
    "        data_string = data_string.decode('utf-8')\n",
    "\n",
    "    json_strings = data_string.strip().split('\\n')\n",
    "    json_objects = []\n",
    "\n",
    "    for json_str in json_strings:\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            json_objects.append(json_obj)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "\n",
    "    return json_objects\n",
    "\n",
    "def monitor_job_status(client, job_id, task_type):\n",
    "    all_completed = False\n",
    "\n",
    "    while not all_completed:\n",
    "        all_completed = True\n",
    "        output_lines = []\n",
    "\n",
    "        updated_job = client.batches.retrieve(job_id)\n",
    "\n",
    "        if updated_job.status.lower() != \"completed\":\n",
    "            all_completed = False\n",
    "            completed = updated_job.request_counts.completed\n",
    "            total = updated_job.request_counts.total\n",
    "            output_lines.append(f\"{task_type.capitalize()} job status: {updated_job.status} - Progress: {completed}/{total}\")\n",
    "        else:\n",
    "            output_lines.append(f\"{task_type.capitalize()} job completed!\")\n",
    "\n",
    "        # Clear the output and display updated status\n",
    "        clear_output(wait=True)\n",
    "        for line in output_lines:\n",
    "            display(line)\n",
    "\n",
    "        if not all_completed:\n",
    "            time.sleep(10)\n",
    "\n",
    "monitor_job_status(client=client, job_id=job.id, task_type='assistant')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TkkhIG9HU0D9",
   "metadata": {
    "id": "TkkhIG9HU0D9"
   },
   "source": [
    "#### Get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "caA5djbLrJ4O",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job = client.batches.retrieve(job.id)\n",
    "result_file_id = batch_job.output_file_id\n",
    "result = client.files.content(result_file_id).content\n",
    "results = parse_json_objects(result)\n",
    "answers = []\n",
    "\n",
    "for res in results:\n",
    "    task_id = res['custom_id']\n",
    "    result = res['response']['body']['choices'][0]['message']['content']\n",
    "    answers.append(result) \n",
    "\n",
    "df['predicted_genre'] = answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68b9c6-5d0a-4641-bc89-cbe432656ea2",
   "metadata": {},
   "source": [
    "## 2. LLM as a Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e74823-addd-480a-925b-a90198db62d3",
   "metadata": {},
   "source": [
    "Now that we identified what are kind of mistakes the LLM is doing, we'll modify the original prompt to help it perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18327c55-35ad-44a5-bdb2-eae0cbb63cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE_PROMPT = '''\n",
    "    You will be provided with a movie description, a list of possible genres, and a predicted movie genre made by another LLM. Your task is to evaluate whether the predicted genre is ‘correct’ or ‘incorrect’ based on the following steps and requirements.\n",
    "    \n",
    "    Steps to Follow:\n",
    "    1. Carefully read the movie description.\n",
    "    2. Determine your own classification of the genre for the movie. Do not rely on the LLM's answer since it may be incorrect. Do not rely on individual words to identify the genre; read the whole description to identify the genre.\n",
    "    3. Read the LLM answer (enclosed in double quotes) and evaluate if it is the correct answer by following the Evaluation Criteria mentioned below.\n",
    "    4. Provide your evaluation as 'correct' or 'incorrect'.\n",
    "    \n",
    "    Evaluation Criteria:\n",
    "    - Ensure the LLM answer (enclosed in double quotes) is one of the provided genres. If it is not listed, the evaluation should be ‘incorrect’.\n",
    "    - If the LLM answer (enclosed in double quotes) does not align with the movie description, the evaluation should be ‘incorrect’.\n",
    "    - The first letter of the LLM answer (enclosed in double quotes) must be capitalized (e.g., Drama). If it has any other capitalization, the evaluation should be ‘incorrect’.\n",
    "    - All other letters in the LLM answer (enclosed in double quotes) must be lowercase. Otherwise, the evaluation should be ‘incorrect’.\n",
    "    - If the LLM answer consists of multiple words, the evaluation should be ‘incorrect’.\n",
    "    - If the LLM answer includes punctuation, spaces, or additional characters, the evaluation should be ‘incorrect’.\n",
    "    \n",
    "    Output Rules:\n",
    "    - Provide the evaluation with no additional text, punctuation, or explanation.\n",
    "    - The output should be in lowercase.\n",
    "    \n",
    "    Final Answer Format:\n",
    "    evaluation\n",
    "    \n",
    "    Example:\n",
    "    correct\n",
    "    '''\n",
    "\n",
    "task_list = create_tasks(df, task_type='judge', system_prompt=JUDGE_PROMPT, model=\"klusterai/Meta-Llama-3.1-405B-Instruct-Turbo\")\n",
    "filename = save_tasks(task_list, task_type='judge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e769fc4-33c5-401b-bb93-eb3d2f83c37c",
   "metadata": {
    "id": "xArKu7-sqSiR"
   },
   "source": [
    "#### Upload Batch File to kluster.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "337e3e75-21ae-4e5e-91de-eb637a0f9b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batch job for batch_tasks_judge.jsonl\n"
     ]
    }
   ],
   "source": [
    "job = create_batch_job(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7050ca-bd2c-4a9e-9a0e-84deaa24480c",
   "metadata": {
    "id": "e-ujphILqepu"
   },
   "source": [
    "#### Check job progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05da7940-0d92-4f1d-80e8-f9bf540c0445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Judge job completed!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "monitor_job_status(client=client, job_id=job.id, task_type='judge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd3a10-7276-42cc-81c6-0456ece42757",
   "metadata": {
    "id": "TkkhIG9HU0D9"
   },
   "source": [
    "#### Get the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "243b7784-ebf0-4d58-a859-73dc08dc2212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge-determined accuracy:  0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "batch_job = client.batches.retrieve(job.id)\n",
    "result_file_id = batch_job.output_file_id\n",
    "result = client.files.content(result_file_id).content\n",
    "results = parse_json_objects(result)\n",
    "evaluations = []\n",
    "\n",
    "for res in results:\n",
    "    task_id = res['custom_id']\n",
    "    result = res['response']['body']['choices'][0]['message']['content']\n",
    "    evaluations.append(result)\n",
    "\n",
    "df['judge_evaluation'] = evaluations\n",
    "\n",
    "print('LLM Judge-determined accuracy: ',df['judge_evaluation'].value_counts(normalize=True)['correct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5a38e-a9aa-457f-92b4-e9cbf5af810f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ecd20b-fda8-4ad4-bd30-bf4d434ee469",
   "metadata": {},
   "source": [
    "According to the LLM Judge, the accuracy of the baseline model was 82%. This demonstrates how, in situations where we lack a ground truth, we can leverage a large-language model to evaluate the responses of another model. By doing so, we can establish a form of ground truth or an evaluation metric that allows us to assess model performance, refine prompts, or understand how well the model is performing overall.\n",
    "\n",
    "This approach is particularly valuable when dealing with large datasets containing thousands of entries, where manual evaluation would be impractical. Automating this process not only saves significant time but also reduces costs by eliminating the need for extensive human annotations. Ultimately, it provides a scalable and efficient way to gain meaningful insights into model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62503e26-6f37-4b61-b920-474d1eccf893",
   "metadata": {},
   "source": [
    "## (Optional) Validation against ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95fb48d-79a8-4f3c-a85f-8ba5c0dda486",
   "metadata": {},
   "source": [
    "According to the LLM Judge, the accuracy of the baseline model is 82%. But how accurate is this evaluation? In this particular case, the IMDb Top 1000 dataset provides ground truth labels, allowing us to directly calculate the accuracy of the predicted genres. Let’s compare and see how close the results are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31503346-67e8-4e16-a44a-1bc91f67bceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM ground truth accuracy:  0.7866666666666666\n"
     ]
    }
   ],
   "source": [
    "print('LLM ground truth accuracy: ',df.apply(lambda row: row['predicted_genre'] in row['Genre'].split(', '), axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c2e90-8fa1-4861-87f7-cf09a52cd25a",
   "metadata": {},
   "source": [
    "Although the ground truth accuracy is not exactly identical to the evaluation provided by the LLM Judge, in situations where we lack ground truth, using an LLM as an evaluator offers a valuable way to assess how well our baseline model is performing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
